{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hohochlo/Pneumonia-Detection-Model/blob/main/Chloe_Kim_PneumoniaDetection_Section3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiM6gYg0nhkY"
      },
      "source": [
        "<font color=\"#de3023\"><h1><b>WELCOME TO THE FINAL SECTION OF THE PNEUMONIA DETECTION PROJECT\n",
        "\n",
        "BY CHLOE KIM</b></h1></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPcvBlSiLXng"
      },
      "source": [
        "In this notebook, we'll try out our model on field data - data outside our normal testing and training data! We'll also learn techniques for data augmentation, creating new \"fake data\" so that our model can generalize more effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsa9kzHFh4yU"
      },
      "source": [
        "In this notebook we'll be:\n",
        "1.   Putting our ML models into practice\n",
        "2.   Improving our ML models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFWVfZctvmPS",
        "outputId": "d18a272b-bac7-4a22-e529-9f9884834751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "import random\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D, Input\n",
        "from keras.regularizers import l2\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.densenet import DenseNet121\n",
        "\n",
        "from imgaug import augmenters\n",
        "def augment(data, augmenter):\n",
        "  if len(data.shape) == 3:\n",
        "    return augmenter.augment_image(data)\n",
        "  if len(data.shape) == 4:\n",
        "    return augmenter.augment_images(data)\n",
        "\n",
        "def rotate(data, rotate):\n",
        "  fun = augmenters.Affine(rotate = rotate)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def shear(data, shear):\n",
        "  fun = augmenters.Affine(shear = shear)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def scale(data, scale):\n",
        "  fun = augmenters.Affine(scale = scale)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def flip_left_right(data, prob):\n",
        "  fun = augmenters.Fliplr(p = prob)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def flip_up_down(data, prob):\n",
        "  fun = augmenters.Flipud(p = prob)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def remove_color(data, channel):\n",
        "  new_data = data.copy()\n",
        "  if len(data.shape) == 3:\n",
        "    new_data[:,:,channel] = 0\n",
        "    return new_data\n",
        "  if len(data.shape) == 4:\n",
        "    new_data[:,:,:,channel] = 0\n",
        "    return new_data\n",
        "\n",
        "class pkg:\n",
        "  #### DOWNLOADING AND LOADING DATA\n",
        "  def get_metadata(metadata_path, which_splits = ['train', 'test']):\n",
        "    '''returns metadata dataframe which contains columns of:\n",
        "       * index: index of data into numpy data\n",
        "       * class: class of image\n",
        "       * split: which dataset split is this a part of?\n",
        "    '''\n",
        "    metadata = pd.read_csv(metadata_path)\n",
        "    keep_idx = metadata['split'].isin(which_splits)\n",
        "    return metadata[keep_idx]\n",
        "\n",
        "  def get_data_split(split_name, flatten, all_data, metadata, image_shape):\n",
        "    '''\n",
        "    returns images (data), labels from folder of format [image_folder]/[split_name]/[class_name]/\n",
        "    flattens if flatten option is True\n",
        "    '''\n",
        "    sub_df = metadata[metadata['split'].isin([split_name])]\n",
        "    index  = sub_df['index'].values\n",
        "    labels = sub_df['class'].values\n",
        "    data = all_data[index,:]\n",
        "    if flatten:\n",
        "      data = data.reshape([-1, np.product(image_shape)])\n",
        "    return data, labels\n",
        "\n",
        "  def get_train_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('train', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_test_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('test', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_field_data(flatten, all_data, metadata, image_shape):\n",
        "    field_data, field_labels = get_data_split('field', flatten, all_data, metadata, image_shape)\n",
        "    field_data[:,:,:,2] = field_data[:,:,:,0]\n",
        "    field_data[:,:,:,1] = field_data[:,:,:,0]\n",
        "\n",
        "    #make data messier\n",
        "    rand = random.uniform(-1, 1)\n",
        "\n",
        "    for i in range(len(field_data)):\n",
        "      image = field_data[i]\n",
        "\n",
        "      if abs(rand) < 0.5:\n",
        "        image = rotate(image, rotate = rand * 40)\n",
        "      elif abs(rand) < 0.8:\n",
        "        image = shear(image, shear = rand*40)\n",
        "      field_data[i] = image\n",
        "    return field_data, field_labels\n",
        "\n",
        "class helpers:\n",
        "  #### PLOTTING\n",
        "  def plot_one_image(data, labels = [], index = None, image_shape = [64,64,3]):\n",
        "    '''\n",
        "    if data is a single image, display that image\n",
        "\n",
        "    if data is a 4d stack of images, display that image\n",
        "    '''\n",
        "    num_dims   = len(data.shape)\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    # reshape data if necessary\n",
        "    if num_dims == 1:\n",
        "      data = data.reshape(target_shape)\n",
        "    if num_dims == 2:\n",
        "      data = data.reshape(np.vstack[-1, image_shape])\n",
        "    num_dims   = len(data.shape)\n",
        "\n",
        "    # check if single or multiple images\n",
        "    if num_dims == 3:\n",
        "      if num_labels > 1:\n",
        "        print('Multiple labels does not make sense for single image.')\n",
        "        return\n",
        "\n",
        "      label = labels\n",
        "      if num_labels == 0:\n",
        "        label = ''\n",
        "      image = data\n",
        "\n",
        "    if num_dims == 4:\n",
        "      image = data[index, :]\n",
        "      label = labels[index]\n",
        "\n",
        "    # plot image of interest\n",
        "    print('Label: %s'%label)\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "  #### QUERYING AND COMBINING DATA\n",
        "  def combine_data(data_list, labels_list):\n",
        "    return np.concatenate(data_list, axis = 0), np.concatenate(labels_list, axis = 0)\n",
        "\n",
        "  def plot_acc(history, ax = None, xlabel = 'Epoch #'):\n",
        "    # i'm sorry for this function's code. i am so sorry.\n",
        "    history = history.history\n",
        "    history.update({'epoch':list(range(len(history['val_accuracy'])))})\n",
        "    history = pd.DataFrame.from_dict(history)\n",
        "\n",
        "    best_epoch = history.sort_values(by = 'val_accuracy', ascending = False).iloc[0]['epoch']\n",
        "\n",
        "    if not ax:\n",
        "      f, ax = plt.subplots(1,1)\n",
        "    sns.lineplot(x = 'epoch', y = 'val_accuracy', data = history, label = 'Validation', ax = ax)\n",
        "    sns.lineplot(x = 'epoch', y = 'accuracy', data = history, label = 'Training', ax = ax)\n",
        "    ax.axhline(0.5, linestyle = '--',color='red', label = 'Chance')\n",
        "    ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')\n",
        "    ax.legend(loc = 4)\n",
        "    ax.set_ylim([0.4, 1])\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel('Accuracy (Fraction)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class models:\n",
        "  def DenseClassifier(hidden_layer_sizes, nn_params):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(nn_params['input_shape']))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    for ilayer in hidden_layer_sizes:\n",
        "      model.add(Dense(ilayer, activation = 'relu'))\n",
        "      model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(units = nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "    model.compile(loss=nn_params['loss'],\n",
        "                  optimizer=keras.optimizers.SGD(learning_rate=1e-4, momentum=0.95),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def CNNClassifier(num_hidden_layers, nn_params):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(nn_params['input_shape']))\n",
        "    model.add(Conv2D(32, (3, 3), padding = 'same', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    for i in range(num_hidden_layers - 1):\n",
        "        model.add(Conv2D(64, (3, 3), padding = 'same', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(units = 128, activation = 'relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units = 64, activation = 'relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(units = nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "\n",
        "    # initiate RMSprop optimizer\n",
        "    opt = keras.optimizers.RMSprop(learning_rate=1e-5)\n",
        "\n",
        "    # Let's train the model using RMSprop\n",
        "    model.compile(loss=nn_params['loss'],\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def TransferClassifier(name, nn_params, trainable = False):\n",
        "    expert_dict = {'VGG16' : VGG16,\n",
        "                   'VGG19' : VGG19,\n",
        "                   'ResNet50' : ResNet50,\n",
        "                   'DenseNet121' : DenseNet121}\n",
        "\n",
        "    expert_conv = expert_dict[name](weights = 'imagenet',\n",
        "                                              include_top = False,\n",
        "                                              input_shape = nn_params['input_shape'])\n",
        "    for layer in expert_conv.layers:\n",
        "      layer.trainable = trainable\n",
        "\n",
        "    expert_model = Sequential()\n",
        "    expert_model.add(expert_conv)\n",
        "    expert_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    expert_model.add(Dense(128, activation = 'relu'))\n",
        "    # expert_model.add(Dropout(0.3))\n",
        "\n",
        "    expert_model.add(Dense(64, activation = 'relu'))\n",
        "    # expert_model.add(Dropout(0.3))\n",
        "\n",
        "    expert_model.add(Dense(nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "\n",
        "    expert_model.compile(loss = nn_params['loss'],\n",
        "                  optimizer = keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return expert_model\n",
        "\n",
        "### defining project variables\n",
        "# file variables\n",
        "metadata_url         = \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20A)%20Pneumonia/metadata.csv\"\n",
        "image_data_url       = 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20A)%20Pneumonia/image_data.npy'\n",
        "image_data_path      = './image_data.npy'\n",
        "metadata_path        = './metadata.csv'\n",
        "image_shape          = (64, 64, 3)\n",
        "\n",
        "# neural net parameters\n",
        "nn_params = {}\n",
        "nn_params['input_shape']       = image_shape\n",
        "nn_params['output_neurons']    = 1\n",
        "nn_params['loss']              = 'binary_crossentropy'\n",
        "nn_params['output_activation'] = 'sigmoid'\n",
        "\n",
        "###\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20A)%20Pneumonia/metadata.csv\"\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20A)%20Pneumonia/image_data.npy\"\n",
        "\n",
        "\n",
        "### pre-loading all data of interest\n",
        "_all_data = np.load('image_data.npy')\n",
        "_metadata = pkg.get_metadata(metadata_path, ['train','test','field'])\n",
        "\n",
        "### preparing definitions\n",
        "# downloading and loading data\n",
        "get_data_split = pkg.get_data_split\n",
        "get_metadata    = lambda :                 pkg.get_metadata(metadata_path, ['train','test'])\n",
        "get_train_data  = lambda flatten = False : pkg.get_train_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_test_data   = lambda flatten = False : pkg.get_test_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_field_data  = lambda flatten = False : pkg.get_field_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "\n",
        "# plotting\n",
        "plot_one_image = lambda data, labels = [], index = None: helpers.plot_one_image(data = data, labels = labels, index = index, image_shape = image_shape);\n",
        "plot_acc       = lambda history: helpers.plot_acc(history)\n",
        "\n",
        "# querying and combining data\n",
        "combine_data           = helpers.combine_data;\n",
        "\n",
        "# models with input parameters\n",
        "DenseClassifier     = lambda hidden_layer_sizes: models.DenseClassifier(hidden_layer_sizes = hidden_layer_sizes, nn_params = nn_params);\n",
        "CNNClassifier       = lambda num_hidden_layers: models.CNNClassifier(num_hidden_layers, nn_params = nn_params);\n",
        "TransferClassifier  = lambda name: models.TransferClassifier(name = name, nn_params = nn_params);\n",
        "\n",
        "monitor = ModelCheckpoint('./model.keras', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metadata.csv        100%[===================>]  39.56K  --.-KB/s    in 0s      \n",
            "image_data.npy      100%[===================>] 131.25M   101MB/s    in 1.3s    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuUfPXfZNf7w"
      },
      "source": [
        "# Milestone 1: Putting our model into practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK4kEOlrNkLh"
      },
      "source": [
        "## Activity 1a: Testing on Field Data\n",
        "\n",
        "While your models may have done well on your original training and validation data, deploying the model on \"field\" data can present different challenges. Field data is data that is different from the one where you built your model (e.g., images obtained from a different x-ray machine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXbX8EwYNn5i"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OcX3HejNrjJ"
      },
      "source": [
        "When we worked with CNNs in the last notebook, we noticed they performed much better than our multi-layered perceptrons on imaging data.\n",
        "\n",
        "Below, please **re-train a 2-layer CNN and plot its accuracy over time!**\n",
        "\n",
        "You can use all the functions from the previous notebook and choose the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLos8L1KnCe3"
      },
      "source": [
        "X_train, y_train = get_train_data()\n",
        "X_test, y_test   = get_test_data()\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aoowqx5Np9n"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpWnKw_IPIr1"
      },
      "source": [
        "Our radiologist friends have provided us with some new data from the field. We can access this with `get_field_data()`.\n",
        "\n",
        "In the code cell below, save the result of calling `get_field_data()` to `X_field` and `y_field`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjyXgqtizcJy"
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zp69Qbd2TGU"
      },
      "source": [
        "What accuracy do you get when you use your trained CNN on the field data? Please calculate it below!\n",
        "\n",
        "Hint: if your predictions are saved as `pred_probabilities`, what do you think something like `pred_probabilities > 0.5` would do? Test it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EhQ-c9L2SsD"
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JncI7hnrPQS5"
      },
      "source": [
        "**Discuss:** How does your performance on field data compare to test and train data?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've probably noticed that you get different performance every time you train your model! Let's get a sense of the average: below, please **train your model 5 times, calculate the accuracy on the field data each time, and print the average accuracy.**"
      ],
      "metadata": {
        "id": "zBcenUENGHBW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7S7rncnQFSP"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf7xfFv9PwrF"
      },
      "source": [
        "## Activity 1b: Understanding our model's performance on field data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdJhwDjBb1U_"
      },
      "source": [
        "### Exercise (Discussion)\n",
        "Discuss this with your instructor:\n",
        "* How did your model do? Did it perform quite as well?\n",
        "* Why do you think it did this way?\n",
        "* **Come up with a few hypotheses for what's different between our test data and our field data!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pILpz1SScRRl"
      },
      "source": [
        "## Activity 1c: Error analysis\n",
        "\n",
        "### Understanding where the model did not perform as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjo7cW5kP14m"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOmLO3M4P3XO"
      },
      "source": [
        "Since our `X_field` comes from the field, it's possible that it's different from `X_train` and `X_test` in some important ways!\n",
        "\n",
        "Let's compare a few images. Below, please use a `for` loop and `plot_one_image(data, labels, index)` to print out a few images from the different datasets.\n",
        "\n",
        "**Do you notice any differences in the datasets?**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIXblKdjSnCe",
        "cellView": "both"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45zKW1NjS4aV"
      },
      "source": [
        "## Instructor-Led Discussion: What is happening in our field data?\n",
        "\n",
        "Discuss in your group and with your instructor:\n",
        "\n",
        "Why is our model performance suffering? What are reasons that field data could be \"messier\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NhR3Mb0Su9U"
      },
      "source": [
        "# Milestone 2: Tools to improve your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhYKVx5nTL2A"
      },
      "source": [
        "## Activity 2a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtnanhTdTS34"
      },
      "source": [
        "### Instructor-Led Discussion: Data Augmentation\n",
        "\n",
        "We want our model to handle a broader variety of input data. One way to do this is to **augment** our data: by intentionally making some alterations to our input data, we can train our model to handle a greater variety of outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riq7peVmkUhA"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ-CtwWseVu_"
      },
      "source": [
        "So, how do we augment our images in Python?\n",
        "\n",
        "\n",
        "We provide custom functions for augmenting a single image. Here's one example, to rotate a single image by 40 degrees. **Experiment with changing the degrees! How can you rotate left?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAUxFajRBeC0"
      },
      "source": [
        "image = X_train[0]\n",
        "plot_one_image(image)\n",
        "new_image = rotate(image, rotate = 40)\n",
        "plot_one_image(new_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1lhczlEHg5y"
      },
      "source": [
        "\n",
        "Here's some more image transformation options:\n",
        "* `rotate(image, rotate = 30)`\n",
        "* `scale(image, scale = 1.5)`\n",
        "* `shear(image, shear = 20)`\n",
        "* `flip_left_right(image, prob = 0.5)`\n",
        "* `flip_up_down(image, prob = 0.5)`\n",
        "* `remove_color(image, channel = 0)`\n",
        "\n",
        "Try it out below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5FcaWcBHkTj"
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LN3ZQkTBdND"
      },
      "source": [
        "Now, experiment with these options!\n",
        "\n",
        "**Share some of your cool augmentation strategies with the class! You can definitely use multiple augmentation techniques for each image!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOEkYlf5gquP"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihblU152Wapd"
      },
      "source": [
        "## Activity 2b."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfVDv1P3abu_"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL9yThE1IZ79"
      },
      "source": [
        "Now, let's create some augmented datasets of our own! We'll combine the augmented data with the original. Here's how you make an augmented dataset with a custom function we've defined for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3kx7OWyIh4k"
      },
      "source": [
        "train_data_rotated_10 = rotate(X_train, rotate=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPZ5862WI-rP"
      },
      "source": [
        "Please make some more augmented datasets below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSa-lSC4JDxO"
      },
      "source": [
        "### YOUR CODE HERE to create more augmented datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNX_jC9eIJFL"
      },
      "source": [
        "\n",
        "\n",
        "Finally, to combine your original train data with your augmented data, you can use the `combine_data` function like this:\n",
        "```\n",
        "all_data, all_labels = combine_data([data1, data2], [labels1, labels2])\n",
        "```\n",
        "\n",
        "(You can include as many datasets as you like in the list. What should you use for the new labels?)\n",
        "\n",
        "Please create `all_data` and `all_labels` below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C7yqCoAW5qs"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4y0BFbJYf1k"
      },
      "source": [
        "Once you've created your augmented data...\n",
        "1. Train your CNN on `all_data`\n",
        "2. Choose the best epoch based on the `X_test`\n",
        "3. Load your model up and score it on `X_field`\n",
        "\n",
        "**Your challenge is to find a set of augmentations that improves your model's performance on the `X_field`! Share your augmentations and performances with the class! Try as many or as few augmentations as you want.**\n",
        "\n",
        "**Remember to record an average of 5 newly initialized CNNs. This is important because CNN weights will be initialized differently in each run!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v453qnBB-Y9e"
      },
      "source": [
        "X_train, y_train = get_train_data()\n",
        "X_test, y_test   = get_test_data()\n",
        "X_field, y_field   = get_field_data()\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz1pSff8xXWk"
      },
      "source": [
        "**Discussion Question**: Why does data augmentation improve average performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Your Model for a Web App\n",
        "\n",
        "Now that you've finished up with training your models on augmented data, you can use the cell below to save the model for use with a web app in the next notebook! Please note the following when using this cell:\n",
        "\n",
        "- This cell saves whatever is in the `cnn` variable, so if your model is in a different variable, change `cnn` in the last line in the code cell!\n",
        "- This cell will save your model to a file in your Google Drive. You're free to change the file name below; just make sure it ends in `.keras`!\n",
        "  - If you're saving multiple different models, you'll definitely want to change the file name between saves. Make sure the name is descriptive so you remember which file is which!"
      ],
      "metadata": {
        "id": "OAeuTBb2ygUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this to save your CNN model!\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "file_name = \"cnn_model_pneumonia_detection.keras\" #@param {type:\"string\"}\n",
        "save_path = \"/content/gdrive/MyDrive/\" + file_name\n",
        "\n",
        "cnn.save(save_path)"
      ],
      "metadata": {
        "id": "3Aa4Y5rvlaGm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE5Xu9QRKydm"
      },
      "source": [
        "##Congratulations on completing your model!\n",
        "\n",
        "By augmenting your data, you've (hopefully) gotten your models working better, even on messy field data!\n",
        "\n",
        "As a further challenge, you can return to any point in the challenge and try to improve:\n",
        "- Try looking at the CNN code in the hidden cell at the top. Can you change the parameters to reduce overfitting?\n",
        "- Try out different classifiers from Scikit-learn with augmented data.\n",
        "- Create new combinations of augmentations for your data.\n",
        "- Whatever ideas you come up with!"
      ]
    }
  ]
}